{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################################################################\n",
    "## Script: AGOLSQLSync.py\n",
    "## Author: Jeremy Mullins, Derek Morgan\n",
    "## Date: 06/05/2019\n",
    "##\n",
    "## Description:\n",
    "##        This script is the test script for syncing data between ArcGIS Online\n",
    "##        and an MS SQL Server database.\n",
    "##\n",
    "## Required prerequisites:\n",
    "##        - ArcGIS API for Python\n",
    "##            (https://developers.arcgis.com/python/guide/install-and-set-up/)\n",
    "##\n",
    "##        - pyodbc Module\n",
    "##            (https://github.com/mkleehammer/pyodbc)\n",
    "##                to install after download, open Python command\n",
    "##                prompt and type the following:\n",
    "##                    -- conda install pyodbc  --\n",
    "##\n",
    "##        - pandas Module\n",
    "##            (https://pandas.pydata.org/pandas-docs/stable/)\n",
    "##                to install after download, open Python command\n",
    "##                prompt and type the following:\n",
    "##                    -- conda install pandas --\n",
    "##\n",
    "##        - MS ODBC Driver for SQL Server (v17)\n",
    "##            (https://docs.microsoft.com/en-us/sql/connect/odbc/download-odbc-driver-for-sql-server?view=sql-server-2017)\n",
    "##\n",
    "############################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################################\n",
    "## SECTION 1: ACCESSING FEATURE LAYER DATA, PREPARE CSV FOR SQL DATABASE ##\n",
    "###########################################################################\n",
    "\n",
    "# import all necessary modules\n",
    "from arcgis.gis import GIS\n",
    "from arcgis import geometry\n",
    "from copy import deepcopy\n",
    "import pyodbc as sql\n",
    "import pandas as pd\n",
    "import configparser\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "# read config file\n",
    "config = configparser.ConfigParser()\n",
    "config.read('config_jm.ini')\n",
    "\n",
    "# assign config variables\n",
    "agolURL = config['AGOL']['URL']\n",
    "agolUSER = config['AGOL']['USER']\n",
    "agolPW = config['AGOL']['PW']\n",
    "sqlDRVR = config['SQL']['SQLDRVR']\n",
    "sqlSERV = config['SQL']['SERVER']\n",
    "sqlDB = config['SQL']['DB']\n",
    "sqlUSER = config['SQL']['USER']\n",
    "sqlPW = config['SQL']['PW']\n",
    "iD = config['SCRIPT']['itemID']\n",
    "csvNAME = config['SCRIPT']['csvTITLE']\n",
    "csvLOC = config['SCRIPT']['csvLOC']\n",
    "zipLOC = config['SCRIPT']['zipLOC']\n",
    "csvDOC = config['SCRIPT']['csvDOC']\n",
    "newTEMP1 = config['SCRIPT']['newTEMP1']\n",
    "newTEMP2 = config['SCRIPT']['newTEMP2']\n",
    "csv2TEMP = config['SCRIPT']['csv2TEMP']\n",
    "newINS1 = config['SCRIPT']['newINS1']\n",
    "newINS2 = config['SCRIPT']['newINS2']\n",
    "SQL2CSV = config['SCRIPT']['SQL2CSV']\n",
    "sql2CSVout = config['SCRIPT']['sql2CSVout']\n",
    "delTEMP1 = config['SCRIPT']['delTEMP1']\n",
    "delTEMP2 = config['SCRIPT']['delTEMP2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sign into AGOL acct\n",
    "gis = GIS(agolURL,agolUSER,agolPW)\n",
    "\n",
    "# get feature layer in question\n",
    "featureLayer = gis.content.get(iD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export feature layer as CSV (ZIP file)\n",
    "output_file = featureLayer.export(title=csvNAME,export_format=\"CSV\")\n",
    "output_file.download(csvLOC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unzip downloaded ZIP folder containing feature layer as CSV\n",
    "zip_ref = zipfile.ZipFile(zipLOC,'r')\n",
    "zip_ref.extractall(csvLOC)\n",
    "zip_ref.close()\n",
    "\n",
    "# delete ZIP folder on disk and CSV collection file in AGOL\n",
    "os.remove(zipLOC)\n",
    "output_file.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read CSV using panda; replace NaNs with '000'\n",
    "df = pd.read_csv(csvDOC)\n",
    "df.fillna('000',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################\n",
    "## SECTION 2: CONNECT TO SQL, PERFORM DATA ANALYSIS ##\n",
    "######################################################\n",
    "\n",
    "#Connect to SQL db and assign cursor\n",
    "conn = sql.connect('Driver='+sqlDRVR+';'\n",
    "                      'Server='+sqlSERV+';'\n",
    "                      'Database='+sqlDB+';'\n",
    "                      'trusted_connection=yes;'\n",
    "                      'UID='+sqlUSER+';'\n",
    "                      'PWD='+sqlPW+';')\n",
    "cursor = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create temporary tables in SQL db\n",
    "cursor.execute(newTEMP1)\n",
    "cursor.commit()\n",
    "\n",
    "cursor.execute(newTEMP2)\n",
    "cursor.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert CSV into temporary table\n",
    "for index,row in df.iterrows():\n",
    "    cursor.execute(csv2TEMP,row['FID'],\n",
    "                   row['BUILDINGID'],\n",
    "                   row['Descriptio'],\n",
    "                   row['Classrooms'],\n",
    "                   row['TypeCode'],\n",
    "                   row['email'],\n",
    "                   row['x'],\n",
    "                   row['y']\n",
    "                  )\n",
    "cursor.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare tables and look for non-duplicated FIDs\n",
    "cursor.execute(newINS1)\n",
    "cursor.commit()\n",
    "\n",
    "# compare tables and look for any updated records in prod table\n",
    "cursor.execute(newINS2)\n",
    "cursor.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################################\n",
    "## SECTION 3: CONVERT SQL UPDATES TO CSV, PUSH TO AGOL, PERFORM UPDATES ##\n",
    "##########################################################################\n",
    "\n",
    "# export SQL data to CSV\n",
    "outCSVscript = SQL2CSV\n",
    "\n",
    "df2 = pd.read_sql_query(outCSVscript,conn)\n",
    "\n",
    "df2.to_csv(sql2CSVout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the SQL update CSV\n",
    "csv2 = sql2CSVout\n",
    "updates_df_2 = pd.read_csv(csv2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query all features in target AGOL feature layer\n",
    "fl = featureLayer.layers[0]\n",
    "flquery = fl.query()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine which features overlap between update CSV and AGOL feature layer using an INNER join\n",
    "overlap_rows = pd.merge(left = flquery.sdf, right = updates_df_2, how = 'inner', on = 'FID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list containing corrected features (ftbu)\n",
    "updatefeatures = []\n",
    "all_features = flquery.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for loop to prepare updated geometries and attributes for each of the updated features\n",
    "for fid in overlap_rows['FID']:\n",
    "    # get the feature to be updated\n",
    "    original_feature = [f for f in all_features if f.attributes['FID'] == fid][0]\n",
    "    ftbu = deepcopy(original_feature)\n",
    "    \n",
    "    # get the matching row from csv\n",
    "    matching_row = updates_df_2.where(updates_df_2.FID == fid).dropna()\n",
    "    \n",
    "    # get geometries in the destination coordinate system\n",
    "    input_geometry = {'y':float(matching_row['Y']),\n",
    "                      'x':float(matching_row['X'])}\n",
    "    output_geometry = geometry.project(geometries = [input_geometry],\n",
    "                                       in_sr = 4326,\n",
    "                                       out_sr = flquery.spatial_reference['latestWkid'],\n",
    "                                       gis = gis)\n",
    "    # assign the updated values\n",
    "    ftbu.geometry = output_geometry[0]\n",
    "    ftbu.attributes['BUILDINGID'] = matching_row['BuildingID'].values[0]\n",
    "    ftbu.attributes['Classrooms'] = matching_row['Classrooms'].values[0]\n",
    "    ftbu.attributes['Descriptio'] = matching_row['Description'].values[0]\n",
    "    ftbu.attributes['TypeCode'] = matching_row['TypeCode'].values[0]\n",
    "    ftbu.attributes['FID'] = int(matching_row['FID'])\n",
    "    ftbu.attributes['email'] = matching_row['email'].values[0]\n",
    "    \n",
    "    # add this to the list of features to be updated\n",
    "    updatefeatures.append(ftbu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call the edit_features() method of FeatureLayer object and pass features to the updates parameter\n",
    "if len(updatefeatures) > 0:\n",
    "    fl.edit_features(updates = updatefeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################\n",
    "## SECTION 4: CLEAN UP DATA WITHIN SQL AND LOCAL MACHINE ##\n",
    "###########################################################\n",
    "\n",
    "# delete temporary table\n",
    "cursor.execute(delTEMP1)\n",
    "cursor.commit()\n",
    "\n",
    "cursor.execute(delTEMP2)\n",
    "cursor.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# close and delete cursor; close SQL db connection\n",
    "cursor.close()\n",
    "del cursor\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all CSVs\n",
    "os.remove(csvDOC)\n",
    "os.remove(sql2CSVout)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
